# Generic Backend for everybody

## Preliminaries
Since this code is part of the Semper-KI platform, it has been written with this particular goal in mind. Nevertheless, stuff like file handling, oauth, and ID management are necessary for almost all platforms. Hence this repository. 

## Environments
In order to get this working, you'll need an environment file (so called `env`-file) for your particular software. You can generate an empty .env file with ```python manage.py generate_env``` (scroll down a bit). The output should be copied into a file lying in your project root folder. Then fill in your specific keys.

We use the following services:
Local:
- postgres as database
- pgadmin to look into the database
- redis as key-value store
- localstack as a local S3 compatible cloud

Remote:
- Digital Ocean Spaces as remote S3 compatible cloud storage
- Auth0 for ID Management
- CMEM for onthology stuff (not necessary for this code)
- The InfAI E-Mail provider

There will probably be more keys printed than you need, just leave them empty, delete them or use some dummy string.

After you filled in the necessary details, create 5 .env files in the root directory of this code: `.env.dev`, `.env.local`, `.env.local_container`, `.env.production`, `.env.stage` and copy these details in there. Change internal connections hosts (e.g. database, redis) to localhost as well as ENV_TOKEN to local to see which env file is being used in outputs.
These will then be used when running the containers or locally. `.env.dev` for example will be used if you debug via VS Code. If only the docker containers are of interest, `.env.local_container` is used. See the next section how to install and launch it.

## Installation for dev purposes
(hint: on windows use the .bat version on linux the .sh version)

- check, that you have at least Python 3.11 installed (via Terminal/Powershell and `python --version`) and correctly linked in your system environment variables (see: https://realpython.com/add-python-to-path/)
- for clean initialization check that the `postgres` folder is empty as well as `redis` folder (if they already exist, if not they should be generated by the containers)
- To run the backend with local installations of all packages:
  - call ```python -m pip install -r requirements.txt``` to install packages to your local machine 
  - call ```start_local_dev.bat -m local ``` to build and run only the containers with background connections (database, redis, ...)
  - call ```python manage.py create_db --env local``` to create the database named in .env.local (which should be the same as in .env.local_container) 
  - call ```python manage.py migrate --env local``` to migrate the database to the latest state 
  - now you can call ```python manage.py runserver --env local``` to start the backend locally and edit 
  - or use VS Code and RUN->Start Debugging but beforehand, stop all containers with ```stop_local_dev.bat```
- To just start the containers without installing anything aside from docker:
  - launch the containerized version via ```start_local_dev.bat -m local_container```
- It worked if http://127.0.0.1:8000 gives you a hint that there is nothing to see there :D

INFO: The documentation can be seen via the private/doc path

## Debug logging
In order to have debug output in the console, in your .env.[MODE] file set ```DJANGO_LOG_LEVEL=DEBUG```.
To log your messages use ```getLogger("django_debug").debug("your message")```

## Docker files
There are a couple of docker and docker-compose files in the root folder. 
Regarding the docker files:
- `Dockerfile`: The main file with a default entry point. Used by almost all docker-compose files
- `Dockerfile.Testing`: Only used for running the tests and is coupled with the docker-compose.test.yml file.

As for the compose files:
- `docker-local-dev-container-backend.yml`: For the backend container when running in local_container mode
- `docker-local-dev-services.yml`: Every other container like redis, postgres and so on
- `docker-compose.test.yml`: For running the tests, can be called via docker-compose up directly
- `docker-compose.staging.yml`: Used on the server for staging
- `docker-compose.production.yml`: Same as above albeit for production

## Optional commands
- ```python manage.py generate_env``` to output an example env file with default values, use with "-p --env <environment>" to get see the values currently used in django
- ```python manage.py create_db --env <environment>``` to create the database named in <environment> - which for now should be "local"
- ```python manage.py check --env <environment>``` to check if the parameters are set, the database can be reached, redis can be reached
- ```python manage.py mail --env <environment> email-address``` to send a test mail to the email-address

## Good to know
- The Backend Container supports hot reloading, which means that editing files and saving changes will be reflected instantly. For the Debug Version, the current handler must be finished, then the worker will restart after saving the changes. The Container is a bit delayed but the logs show if and when it happened.

